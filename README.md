# Production-Ready Generative AI chatbot using AWS Bedrock & RAG

A Generative AI intelligent assistant using AWS Bedrock & RAG, built applying production-ready principles with its infrastructure managed via IaC (Infrastructure as Code). It enables contextual Q&A from diverse data sources (unstructured/structured). A portfolio project demonstrating robust GenAI application development on AWS.

## Developer Setup

To contribute to this project, ensure the following tools are installed on your machine:

### System Requirements

- [Terraform](https://developer.hashicorp.com/terraform/install)
- [terraform-docs](https://terraform-docs.io/user-guide/installation/)
- [Python 3.11+](https://www.python.org/downloads/)
- [Node.js 18+ and npm](https://nodejs.org/)

### Installation Steps (macOS example)

```bash
brew install terraform
brew install terraform-docs
brew install python
brew install node
brew install rust
```

## Multi-turn Conversation with RAG

The chatbot implements multi-turn conversation capabilities using Amazon Bedrock's Converse API combined with Retrieval Augmented Generation (RAG). This allows the chatbot to maintain context across multiple interactions while providing responses based on your knowledge base.

### How It Works

1. **Intelligent RAG Usage**:
   - Uses a lightweight classifier model to determine if RAG is needed for each query
   - Only queries the knowledge base when external information is required
   - Optimizes cost and latency by avoiding unnecessary knowledge base calls

2. **Combined RAG and Conversation History**:
   - Uses `retrieve_and_generate` to get information from the knowledge base when needed
   - Uses `converse` to generate responses based on conversation history
   - Maintains conversation context between requests

3. **Session Management**:
   - A unique `sessionId` is generated by Amazon Bedrock on the first request
   - The client stores and sends this `sessionId` with subsequent requests
   - Conversation history is maintained in a `messages` array

4. **Request Format**:
   ```json
   {
     "input": "Your question here",
     "sessionId": "previous-session-id",  // Optional for first request
     "messages": []  // Optional for first request
   }
   ```

5. **Response Format**:
   ```json
   {
     "answer": "The model's response",
     "sessionId": "session-id-to-use-next-time",
     "messages": [/* conversation history */],
     "used_rag": true  // Indicates if RAG was used for this query
   }
   ```

6. **Session Expiration**:
   - Amazon Bedrock sessions typically expire after a period of inactivity (usually 30 minutes)
   - No explicit TTL configuration is needed for basic implementation

### Implementation Notes

- No database is required for this implementation as session state is maintained by the client
- For production use with high traffic, consider implementing DynamoDB for server-side session management
- The client application is responsible for storing and sending the session information

# Lambda Chatbot Backend (Bedrock Converse)

This folder contains Python Lambda functions for the AI chatbot backend using AWS Bedrock Converse API.

## Structure

- `src/chatbot.py` — Lambda handler and logic for the chatbot
- `scripts/build.sh` — Script to build and package Lambda(s) for deployment

## Packaging & Deployment

1. **Build the Lambda package:**

   ```zsh
   cd scripts
   chmod +x build.sh
   ./build.sh chatbot
   ```

   This creates `dist/chatbot.zip` ready for upload (e.g., to S3 for Terraform).

2. **Terraform Integration:**

   - Upload `chatbot.zip` to S3 (manually or via CI/CD pipeline).
   - Reference the S3 object in your `aws_lambda_function` resource in Terraform.

3. **API Gateway Integration:**
   - Use Lambda Proxy integration for flexible request/response handling.

## Environment Variables

- `BEDROCK_KB_ID` — The Knowledge Base ID for Bedrock Converse.
- `BEDROCK_MODEL_ARN` — The ARN of the model used for knowledge base retrieval.
- `BEDROCK_MODEL_ID` — The model ID used for conversation (defaults to Claude 3 Sonnet if not specified).
- `BEDROCK_CLASSIFIER_MODEL_ID` — The model ID used for classifying if RAG is needed (defaults to Claude Instant for cost efficiency).

## Notes

- Use [AWS Lambda Powertools](https://awslabs.github.io/aws-lambda-powertools-python/latest/) for logging, metrics, and tracing in production.
- Update `requirements.txt` as needed for additional dependencies.

## Notes

- Use [AWS Lambda Powertools](https://awslabs.github.io/aws-lambda-powertools-python/latest/) for logging, metrics, and tracing in production.
- Update `requirements.txt` as needed for additional dependencies.
